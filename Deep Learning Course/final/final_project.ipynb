{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QLA0tZ6mpMzt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spam or Ham\n",
        "\n",
        "## Final Project - Intro to Deep Learning\n",
        "## BGU Winter 2021"
      ],
      "metadata": {
        "id": "3fJknEjW8snZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK's Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "vX1q9uoNpTPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JMVlQUyv8mLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d628b3c-b659-40ab-fac7-aa58d59070a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from os import walk\n",
        "from string import punctuation\n",
        "from random import shuffle\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import sklearn as sk\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "path_route = os.walk('/content/gdrive/My Drive/deep_learning/FinalProject/data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4c6CxOS10WQ",
        "outputId": "3a9c6822-960b-4716-c355-0d6addb31346"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hamData, spamData = [], []\n",
        "\n",
        "for root, directory, file in path_route:\n",
        "  if 'ham' in str(file):\n",
        "        for obj in file:\n",
        "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
        "                hamData.append(\" \".join(ip.readlines()))\n",
        "  elif 'spam' in str(file):\n",
        "        for obj in file:\n",
        "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
        "                spamData.append(\" \".join(ip.readlines()))"
      ],
      "metadata": {
        "id": "gPvDx0xI16TL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# remove all redundent data\n",
        "\n",
        "allHamData = list(set(hamData))\n",
        "allSpamData = list(set(spamData))\n",
        "\n",
        "# storing it in a dataframe\n",
        "\n",
        "hamPlusSpamData = allHamData + allSpamData\n",
        "labels = [\"ham\"]*len(allHamData) + [\"spam\"]*len(allSpamData)\n",
        "\n",
        "raw_df = pd.DataFrame({\"email\": hamPlusSpamData, \n",
        "                       \"label\": labels})\n",
        "\n",
        "# checking how it looks\n",
        "\n",
        "raw_df.sample(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HV91Yd3I_Vfz",
        "outputId": "857324a1-7f13-42c0-ca88-6aa1e10de48f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a preprocessing function\n",
        "# to tokenize and lemmatize the data using NLTK library\n",
        "\n",
        "def preprocess(data):\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(data)\n",
        "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
        "\n",
        "    # finding uncommon words\n",
        "    cnt = Counter(tokens)\n",
        "    uncommons = cnt.most_common()[:-int(len(cnt)*0.1):-1]\n",
        "    \n",
        "    # listing stopwords from NLTK\n",
        "    stops = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    # removing stop words and uncommon words\n",
        "    tokens = [w for w in tokens if (w not in stops and w not in uncommons)]\n",
        "\n",
        "    # lemmatization\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w, pos='a') for w in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# pre-processing the emails\n",
        "# using word_tokenize() and WordNetLemmatizer()\n",
        "\n",
        "nltk_processed_df = pd.DataFrame()\n",
        "nltk_processed_df['email'] = [preprocess(e) for e in raw_df.email]\n",
        "\n",
        "# label encoding the labels\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "nltk_processed_df['label'] = label_encoder.fit_transform(raw_df.label)\n",
        "\n",
        "# checking how the processed data looks like\n",
        "\n",
        "nltk_processed_df.sample(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTjgpcGqrzcH",
        "outputId": "f8373bfe-dc93-4818-cd68-2179b5773c3b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.label[25526]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dZlTQ-9Wmm1b",
        "outputId": "f8c36254-cd68-4319-fb83-5400ec48d6e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "raw_df.email[25526]\n"
      ],
      "metadata": {
        "id": "7kzeFZswl1yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk_processed_df.label[25526]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgPb51NgmDEh",
        "outputId": "ebc9d23f-bcd8-44c0-cb41-33c280cf7d0f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_processed_df.email[25526]"
      ],
      "metadata": {
        "id": "4WPhnWJmmIkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting categorical email data to numerical data using Counters\n",
        "\n",
        "X, y = nltk_processed_df.email, nltk_processed_df.label\n",
        "X_featurized = [Counter(i) for i in X]"
      ],
      "metadata": {
        "id": "XmSaUVH7me04"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the data ready for NaiveBayesClassifier \n",
        "# randomizing using shuffle\n",
        "# manually splitting into test and train data\n",
        "\n",
        "allDataProcessed = [(X_featurized[i], y[i]) for i in range(len(X))]\n",
        "\n",
        "shuffle(allDataProcessed)\n",
        "\n",
        "trainData, testData = allDataProcessed[:int(len(allDataProcessed)*0.7)], allDataProcessed[int(len(allDataProcessed)*0.7):]"
      ],
      "metadata": {
        "id": "exibvL06m8HH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "\n",
        "model_nltkNaiveBayes = nltk.classify.NaiveBayesClassifier.train(trainData)"
      ],
      "metadata": {
        "id": "jdx9RrYnnqOQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "\n",
        "testing_accuracy = nltk.classify.accuracy(model_nltkNaiveBayes, testData)\n",
        "print(\"Accuracy with NLTK's Naive Bayes classifier is:\", testing_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYzHuzBnn1Kd",
        "outputId": "e71e5a84-a666-4c88-8891-8c83db581507"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with NLTK's Naive Bayes classifier is: 0.9889605421357526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scikit-learn's Multinomial Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "QLA0tZ6mpMzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the features using CountVectorizer\n",
        "\n",
        "cv_vec = sk.feature_extraction.text.CountVectorizer(tokenizer = nltk.word_tokenize, \n",
        "                                                    stop_words = nltk.corpus.stopwords.words(\"english\"))\n",
        "\n",
        "cv_X = cv_vec.fit_transform(raw_df.email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_FrEzXkpRbd",
        "outputId": "ddb0c176-cb2e-46b9-8a0d-880daeff6dc2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the features using TfidfVectorizer\n",
        "\n",
        "tfidf_vec = sk.feature_extraction.text.TfidfVectorizer(tokenizer = nltk.word_tokenize, \n",
        "                                                    stop_words = nltk.corpus.stopwords.words(\"english\"))\n",
        "\n",
        "tdidf_X = cv_vec.fit_transform(raw_df.email)"
      ],
      "metadata": {
        "id": "V8SWLGaxqPJi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label encode the labels using LabelEncoder\n",
        "\n",
        "label_encoder = sk.preprocessing.LabelEncoder()\n",
        "y = label_encoder.fit_transform(raw_df.label)"
      ],
      "metadata": {
        "id": "GjUlmZs9q30d"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the MultinomialNB model\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model_sklearn_mnb = MultinomialNB()"
      ],
      "metadata": {
        "id": "TCl8L8n3rHd9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting cross validation score on count-vectorized features\n",
        "# getting cross validation score on tfidf processed features\n",
        "\n",
        "cv_score = sk.model_selection.cross_validate(model_sklearn_mnb, cv_X, y)\n",
        "\n",
        "tfidf_score = sk.model_selection.cross_validate(model_sklearn_mnb, tdidf_X, y)"
      ],
      "metadata": {
        "id": "HhIv-ND5rQTB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the scores by putting them into a dataframe first\n",
        "\n",
        "sklearn_scores = pd.DataFrame([cv_score, tfidf_score], index=['CountVetorizer', 'TfidfVectorizer'])\n",
        "sklearn_scores = sklearn_scores.applymap(lambda x: x.mean())\n",
        "\n",
        "sklearn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "OhPaGXt5rbhY",
        "outputId": "01eeb61e-4749-4111-9047-5306344a9395"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dd0495be-7960-496a-8e8f-b8e58cbf3874\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fit_time</th>\n",
              "      <th>score_time</th>\n",
              "      <th>test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>CountVetorizer</th>\n",
              "      <td>0.050531</td>\n",
              "      <td>0.009494</td>\n",
              "      <td>0.983931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TfidfVectorizer</th>\n",
              "      <td>0.048479</td>\n",
              "      <td>0.009019</td>\n",
              "      <td>0.983931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd0495be-7960-496a-8e8f-b8e58cbf3874')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd0495be-7960-496a-8e8f-b8e58cbf3874 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd0495be-7960-496a-8e8f-b8e58cbf3874');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 fit_time  score_time  test_score\n",
              "CountVetorizer   0.050531    0.009494    0.983931\n",
              "TfidfVectorizer  0.048479    0.009019    0.983931"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}